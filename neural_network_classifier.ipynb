{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Network from Scratch \u2014 Letter Classifier (A, B, C)\n",
        "\n",
        "This notebook implements a small feedforward neural network (one hidden layer) **from scratch** using only NumPy to classify binary 5\u00d76 pixel patterns representing the letters **A**, **B**, and **C**. It trains the network with backpropagation and visualizes loss, accuracy, and predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Each letter is a 5 rows x 6 cols pattern (5x6 = 30 pixels). 1 = ink / white pixel, 0 = background\n",
        "A = [\n",
        "    [0,1,1,1,0,0],\n",
        "    [1,0,0,0,1,0],\n",
        "    [1,1,1,1,1,0],\n",
        "    [1,0,0,0,1,0],\n",
        "    [1,0,0,0,1,0]\n",
        "]\n",
        "\n",
        "B = [\n",
        "    [1,1,1,1,0,0],\n",
        "    [1,0,0,0,1,0],\n",
        "    [1,1,1,1,0,0],\n",
        "    [1,0,0,0,1,0],\n",
        "    [1,1,1,1,0,0]\n",
        "]\n",
        "\n",
        "C = [\n",
        "    [0,1,1,1,1,0],\n",
        "    [1,0,0,0,0,1],\n",
        "    [1,0,0,0,0,0],\n",
        "    [1,0,0,0,0,1],\n",
        "    [0,1,1,1,1,0]\n",
        "]\n",
        "\n",
        "A = np.array(A).reshape(30)\n",
        "B = np.array(B).reshape(30)\n",
        "C = np.array(C).reshape(30)\n",
        "\n",
        "X = np.vstack([A, B, C]).astype(np.float32)\n",
        "y = np.array([[1,0,0], [0,1,0], [0,0,1]], dtype=np.float32)\n",
        "\n",
        "print('X shape:', X.shape)\n",
        "print('y shape:', y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Network hyperparameters\n",
        "input_size = 30\n",
        "hidden_size = 8   # you can tune this\n",
        "output_size = 3\n",
        "learning_rate = 0.5\n",
        "epochs = 5000\n",
        "\n",
        "# Weight initialization (small random values)\n",
        "W1 = np.random.randn(input_size, hidden_size) * 0.1\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size) * 0.1\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(sig_x):\n",
        "    # input is sigmoid(x) already (common trick)\n",
        "    return sig_x * (1 - sig_x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_history = []\n",
        "acc_history = []\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1            # (3, hidden_size)\n",
        "    a1 = sigmoid(z1)                   # hidden activations\n",
        "    z2 = np.dot(a1, W2) + b2           # (3, output_size)\n",
        "    a2 = sigmoid(z2)                   # output activations\n",
        "\n",
        "    # Loss (MSE) across the batch\n",
        "    loss = np.mean((y - a2) ** 2)\n",
        "    loss_history.append(loss)\n",
        "\n",
        "    # Accuracy (simple argmax)\n",
        "    preds = np.argmax(a2, axis=1)\n",
        "    labels = np.argmax(y, axis=1)\n",
        "    acc = np.mean(preds == labels)\n",
        "    acc_history.append(acc)\n",
        "\n",
        "    # Backpropagation\n",
        "    error_output = (y - a2)               # (3, output_size)\n",
        "    delta_output = error_output * sigmoid_derivative(a2)\n",
        "\n",
        "    error_hidden = np.dot(delta_output, W2.T)   # (3, hidden_size)\n",
        "    delta_hidden = error_hidden * sigmoid_derivative(a1)\n",
        "\n",
        "    # Gradients (batch gradients)\n",
        "    dW2 = np.dot(a1.T, delta_output)            # (hidden_size, output_size)\n",
        "    db2 = np.sum(delta_output, axis=0, keepdims=True)\n",
        "    dW1 = np.dot(X.T, delta_hidden)             # (input_size, hidden_size)\n",
        "    db1 = np.sum(delta_hidden, axis=0, keepdims=True)\n",
        "\n",
        "    # Update weights (gradient ascent on squared error: we add because error_output was (y - a2))\n",
        "    W2 += learning_rate * dW2\n",
        "    b2 += learning_rate * db2\n",
        "    W1 += learning_rate * dW1\n",
        "    b1 += learning_rate * db1\n",
        "\n",
        "    if epoch % 500 == 0 or epoch == 1:\n",
        "        print(f'Epoch {epoch}/{epochs} \u2014 loss: {loss:.6f} \u2014 acc: {acc:.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.title('Training Loss over Epochs')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(acc_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training Accuracy over Epochs')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(sample):\n",
        "    z1 = np.dot(sample.reshape(1, -1), W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "    return a2.flatten()\n",
        "\n",
        "labels = ['A', 'B', 'C']\n",
        "for i, s in enumerate(X):\n",
        "    out = predict(s)\n",
        "    pred_idx = np.argmax(out)\n",
        "    print(f'True: {labels[i]}  \u2014 Pred: {labels[pred_idx]}  \u2014 Probabilities: {out.round(3)}')\n",
        "    plt.figure(figsize=(2,3))\n",
        "    plt.imshow(s.reshape(5,6), cmap='gray')\n",
        "    plt.title(f'True: {labels[i]}  Pred: {labels[pred_idx]}')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save trained weights (so you can load later)\n",
        "out_dir = Path('/mnt/data')\n",
        "np.savez(out_dir / 'nn_weights.npz', W1=W1, b1=b1, W2=W2, b2=b2)\n",
        "print('Saved weights to', out_dir / 'nn_weights.npz')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This simple network shows how to implement forward and backward passes using NumPy and train a tiny classifier on handcrafted binary images. For better generalization, you can:\n",
        "- add more training samples (noisy variants),\n",
        "- experiment with different hidden layer sizes and learning rates,\n",
        "- use cross-entropy + softmax for the final layer,\n",
        "- use regularization or momentum.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}